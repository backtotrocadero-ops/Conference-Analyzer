import streamlit as st
import io, re
import pandas as pd

try:
    import fitz  # PyMuPDF
except:
    fitz = None

# ì–¸ì–´ ê°ì§€
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0
    def detect_lang(s):
        try:
            return detect(s)
        except:
            return "en"
except:
    def detect_lang(s):
        return "en"

# OpenAI ìš”ì•½
try:
    import openai
    openai_available = True
except:
    openai_available = False

def simple_summary(text, max_words=25):
    words = re.split(r'\s+', text.strip())
    return " ".join(words[:max_words]) + ("..." if len(words) > max_words else "")

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(uploaded_file):
    data = uploaded_file.read()
    text = ""
    if fitz:
        try:
            doc = fitz.open(stream=data, filetype="pdf")
            for page in doc:
                blocks = page.get_text("blocks")
                for b in blocks:
                    text += b[4].strip() + "\n"
        except:
            text = ""
    return text

# ì„¸ì…˜ ë¶„ì„ (ì‚¬ìš©ì ì •ì˜ ê·œì¹™ ì ìš© + ì¤‘ë³µ ì œê±° + 2ì¹¸ ì´ìƒ ê³µë°± ê¸°ì¤€)
def parse_sessions_from_text(text):
    # 2ì¹¸ ì´ìƒ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¸”ë¡ ë‚˜ëˆ„ê¸°
    blocks = re.split(r'\s{2,}', text)
    sessions = []
    seen_texts = set()  # ì¤‘ë³µ ì œê±°ìš©

    current_time = ""
    current_place = ""
    
    for block in blocks:
        block = block.strip()
        if not block:
            continue
        # ì¤‘ë³µ ì œê±°
        if block in seen_texts:
            continue
        seen_texts.add(block)

        time = ""
        place = ""
        title = ""

        # 1) ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì‹œê°„
        if re.match(r'^\d{1,2}[:.]\d{2}', block):
            current_time = block
            continue  # ì‹œê°„ë§Œ ìˆìœ¼ë©´ ë‹¤ìŒ ë¸”ë¡ì—ì„œ ì œëª©/ì¥ì†Œ ì²˜ë¦¬

        # 2) ì¥ì†Œ í‚¤ì›Œë“œ
        if any(k.lower() in block.lower() for k in ['omega', 'lambda', 'hall']):
            current_place = block

        # 3) ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ë©´ ì œëª©
        if re.match(r'^[A-Z][A-Za-z\s,&\-:]*', block):
            title = block

        # ìµœì†Œ í•œ ê°œë¼ë„ ìˆìœ¼ë©´ ì„¸ì…˜ ì¶”ê°€
        if title or current_time or current_place:
            sessions.append({
                "time": current_time,
                "place": current_place,
                "title": title,
                "text": title if title else block,
                "lang": detect_lang(title if title else block)
            })
            # ì œëª©ì´ ì¶”ê°€ë˜ì—ˆìœ¼ë©´ ë‹¤ìŒ ë¸”ë¡ì—ì„œëŠ” ìƒˆë¡œ ì œëª©/ì¥ì†Œ ì°¾ê¸°
            title = ""
    return sessions

def summarize_with_openai(text):
    if not openai_available or not st.secrets.get("OPENAI_API_KEY"):
        return simple_summary(text)
    try:
        openai.api_key = st.secrets["OPENAI_API_KEY"]
        prompt = f"You are a concise assistant. Summarize this in 1 line:\n{text}"
        resp = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"user","content":prompt}],
            max_tokens=120,
            temperature=0.2,
        )
        return resp['choices'][0]['message']['content'].strip()
    except:
        return simple_summary(text)

# --- Streamlit UI ---
st.set_page_config(page_title="Conference PDF Analyzer", layout="wide")
st.title("ğŸ—‚ï¸ Conference PDF Analyzer")

st.markdown("**ì‚¬ìš©ë²•**: PDF ì—…ë¡œë“œ â†’ ê´€ì‹¬ í‚¤ì›Œë“œ í™•ì¸ â†’ ë¶„ì„ ì‹œì‘")

uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
keywords_input = st.text_input("ê´€ì‹¬ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value="ammunition, defense, NATO, íƒ„ì•½, êµ­ë°©")
use_openai = st.checkbox("OpenAIë¡œ ìš”ì•½ ì‚¬ìš© (ì„ íƒ, Streamlit Secretsì— API_KEY í•„ìš”)", value=False)

if uploaded_file:
    st.info("PDF ë¶„ì„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")
    text = extract_text_from_pdf(uploaded_file)
    if not text.strip():
        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        sessions = parse_sessions_from_text(text)
        st.success(f"ì´ {len(sessions)}ê°œ ì„¸ì…˜ ê°ì§€ë¨")
        keywords = [k.strip().lower() for k in keywords_input.split(",") if k.strip()]
        rows = []
        for s in sessions:
            summary = summarize_with_openai(s['title'] + "\n" + s['text']) if (use_openai and openai_available) else simple_summary(s['title'])
            lang_label = "EN" if s['lang'].startswith("en") else "âš  Not English"
            score = sum(1 for kw in keywords if kw in (s['title'] + " " + s['text']).lower())
            if score >= 2: priority = "â­â­â­ (ê°•ë ¥ì¶”ì²œ)"
            elif score == 1: priority = "â­â­ (ì¶”ì²œ)"
            else: priority = "â­ (ì°¸ê³ )"
            rows.append({
                "ì‹œê°„": s['time'],
                "ì¥ì†Œ": s['place'],
                "ì œëª©": s['title'],
                "í•µì‹¬ìš”ì•½": summary,
                "ì–¸ì–´": lang_label,
                "ìš°ì„ ìˆœìœ„": priority
            })
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True)

        # Excel ë‹¤ìš´ë¡œë“œ
        towrite = io.BytesIO()
        df.to_excel(towrite, index=False, sheet_name="recommendations")
        towrite.seek(0)
        st.download_button("ğŸ“¥ Excelë¡œ ë‹¤ìš´ë¡œë“œ", data=towrite, file_name="conference_recommendations.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

        st.markdown("----")
        st.subheader("ìƒì„¸ ë¦¬ìŠ¤íŠ¸")
        for i, r in df.iterrows():
            st.markdown(f"**{i+1}. [{r['ìš°ì„ ìˆœìœ„']}] {r['ì œëª©']}**  â€”  {r['ì‹œê°„']} / {r['ì¥ì†Œ']}  ({r['ì–¸ì–´']})")
            st.write(r['í•µì‹¬ìš”ì•½'])
            st.write("---")
